{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789a2557-5dbf-43a2-a5d9-a45769a813f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import torch.optim as optim\n",
    "from cleverhans.torch.attacks.projected_gradient_descent import (projected_gradient_descent)\n",
    "\n",
    "import quantus\n",
    "import captum\n",
    "from captum.attr import Saliency, IntegratedGradients, NoiseTunnel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import gc\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from resnet_18 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd58378c-41af-44f0-9c36-50bd3d0aa1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb4934b8-9d2b-46d1-9bc3-a2a4a276957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e91f6e9b-7045-40d4-b041-0c8ad2540529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb\n",
    "%run metrics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70895d69-1cee-4ca4-9eef-19317edbece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_path = '../datasets/imagenette2/train'\n",
    "val_path = '../datasets/imagenette2/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0b6f4c0-9af8-4e2d-a8e0-264ffef0cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(datasets.ImageFolder(train_path, transform = transforms.Compose([\n",
    "                                                                    transforms.RandomResizedCrop(224),\n",
    "                                                                    transforms.RandomHorizontalFlip(),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                                            ])), batch_size = batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "test_dataloader = DataLoader(datasets.ImageFolder(val_path,\n",
    "                                                               transform=transforms.Compose([\n",
    "                                                                   transforms.ToTensor(),\n",
    "                                                                   transforms.Resize([224, 224]),\n",
    "                                                                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                                               ])),batch_size=batch_size, shuffle=True,num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9742a475-96d0-41e8-a6f6-118d2ecdd438",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['tench', 'springer', 'casette_player', 'chain_saw','church', 'French_horn', 'garbage_truck', 'gas_pump', 'golf_ball', 'parachute']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5b653e8-c20c-46a5-b0ae-1c46c5e80305",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e61e5866-0c13-47ce-8f2d-e508484d2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainer_wrapper(**kwargs):\n",
    "    \"\"\"\n",
    "    A wrapper function to call the appropriate explanation method.\n",
    "\n",
    "    \"\"\"\n",
    "    if kwargs[\"method\"] == \"SmoothGrad\":\n",
    "        return smoothgrad_explainer(**kwargs)\n",
    "    else:\n",
    "        return ValueError(\"Explanation function doesnt exist\")\n",
    "\n",
    "\n",
    "\n",
    "def smoothgrad_explainer(model, inputs, targets, abs=True, normalise=True, stdevs=0.15, nt_samples=10, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate explanations for a model's predictions using the SmoothGrad method.\n",
    "\n",
    "    Args:\n",
    "        model: The model to explain.\n",
    "        inputs : Input samples.\n",
    "        targets: Target labels corresponding to the inputs.\n",
    "        abs : Whether to use the absolute value of gradients.\n",
    "        normalise: Whether to normalize the explanation\n",
    "        stdevs: Standard deviation of the noise added to inputs\n",
    "        nt_samples: Number of noisy samples to generate for SmoothGrad.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Explanation maps for the input samples.\n",
    "    \"\"\"\n",
    "    std = kwargs.get(\"std\", 0.15)  # Standard deviation for input noise\n",
    "    n = kwargs.get(\"n\", 10)  # Number of noisy samples\n",
    "    clip = kwargs.get(\"clip\", False)\n",
    "\n",
    "    model.to(kwargs.get(\"device\", None))\n",
    "    model.eval()\n",
    "\n",
    "    if not isinstance(inputs, torch.Tensor):\n",
    "        inputs = (\n",
    "            torch.Tensor(inputs).reshape(-1,kwargs.get(\"nr_channels\", 3),kwargs.get(\"img_size\", 224),kwargs.get(\"img_size\", 224),).to(kwargs.get(\"device\", None))\n",
    "        )\n",
    "    \n",
    "    if not isinstance(targets, torch.Tensor):\n",
    "        targets = torch.as_tensor(targets).long().to(kwargs.get(\"device\", None))\n",
    "\n",
    "    assert (len(np.shape(inputs)) == 4), \"Inputs should be shaped (nr_samples, nr_channels, img_size, img_size\"\n",
    "\n",
    "    if inputs.shape[0] > 1:\n",
    "        explanation = torch.zeros(\n",
    "            (\n",
    "                n,\n",
    "                inputs.shape[0],\n",
    "                kwargs.get(\"img_size\", 224),\n",
    "                kwargs.get(\"img_size\", 224),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        explanation = torch.zeros(\n",
    "            (n, kwargs.get(\"img_size\", 224), kwargs.get(\"img_size\", 224))\n",
    "        )\n",
    "    saliency = Saliency(model)\n",
    "    \n",
    "    explanation = (\n",
    "        NoiseTunnel(saliency)\n",
    "        .attribute(inputs=inputs, target=targets, nt_type=\"smoothgrad\", stdevs = stdevs, nt_samples= 10)\n",
    "        .sum(axis=1)\n",
    "        .reshape(-1, kwargs.get(\"img_size\", 224), kwargs.get(\"img_size\", 224))\n",
    "        .cpu()\n",
    "        .data\n",
    "    )\n",
    "\n",
    "    # explanation = explanation.mean(axis=0)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    explanation = explanation.numpy()\n",
    "    # Normalization (if required)\n",
    "    if normalise:\n",
    "        explanation = quantus.normalise_func.normalise_by_negative(explanation)\n",
    "\n",
    "    # Convert the result to NumPy if it is still a PyTorch tensor\n",
    "    if isinstance(explanation, torch.Tensor):\n",
    "        if explanation.requires_grad:\n",
    "            return explanation.cpu().detach().numpy()\n",
    "        return explanation.cpu().numpy()\n",
    "\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "689c64b4-1a5f-4189-bbed-d86f7e5c2a28",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m xai_method\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSmoothGrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\memory.py:222\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 222\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_emptyCache()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "xai_method=[\"SmoothGrad\"]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4a931-d0b0-462f-9c3f-6448639255f3",
   "metadata": {},
   "source": [
    "## Vanilla Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b06c642-5d45-4cef-8182-4bc401b98d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.15%\n",
      "The road score is:  {1: np.float64(0.9854187562209861), 11: np.float64(0.9302623147929386), 21: np.float64(0.8855254694918366), 31: np.float64(0.8275752431472999), 41: np.float64(0.7686531744591676), 51: np.float64(0.7003263311118203), 61: np.float64(0.6167151750011796), 71: np.float64(0.5189097090335428), 81: np.float64(0.41946263606951006), 91: np.float64(0.30242259808422944)}\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/0_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.eval()\n",
    "accuracy = test(model, test_dataloader, criterion)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"Saliency\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e484718-7ebd-45a7-8466-bcde0291bf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.10%\n",
      "The road score is:  {1: np.float64(0.9837310536786187), 11: np.float64(0.929972869715964), 21: np.float64(0.866104104853331), 31: np.float64(0.8072800443816678), 41: np.float64(0.7171702227608943), 51: np.float64(0.6231640628883724), 61: np.float64(0.5228790319156869), 71: np.float64(0.4260239943928879), 81: np.float64(0.3296042879105286), 91: np.float64(0.2543334457371799)}\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/1_5_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test(model, test_dataloader, criterion)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"Saliency\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b4f2142-8ddc-455c-ac23-62c294672a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.13%\n",
      "The road score is:  {1: np.float64(0.9799208963778208), 11: np.float64(0.9136983714162727), 21: np.float64(0.8637160553598301), 31: np.float64(0.8006494027445334), 41: np.float64(0.7237594767473243), 51: np.float64(0.6276571377706955), 61: np.float64(0.5362060711547837), 71: np.float64(0.43228985890760535), 81: np.float64(0.3291858131864658), 91: np.float64(0.25164405727791406)}\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/1_10_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test(model, test_dataloader, criterion)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"Saliency\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8da56644-948a-4c39-883c-79516ed66ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.18%\n",
      "The road score is:  {1: np.float64(0.9800060568550581), 11: np.float64(0.9197179038239468), 21: np.float64(0.8670742404008076), 31: np.float64(0.8091293294324844), 41: np.float64(0.7264244267648965), 51: np.float64(0.6474040927489494), 61: np.float64(0.5544084061680202), 71: np.float64(0.4606913360362517), 81: np.float64(0.3678827795588879), 91: np.float64(0.2800112830488548)}\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/1_15_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test(model, test_dataloader, criterion)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"Saliency\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f97fb389-179c-46c6-8a86-b2d3e551459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.31%\n",
      "The road score is:  {1: np.float64(0.9815317446919453), 11: np.float64(0.916264309570018), 21: np.float64(0.8550035656800294), 31: np.float64(0.78959464373224), 41: np.float64(0.7143713719788255), 51: np.float64(0.6170017447664645), 61: np.float64(0.535769578517576), 71: np.float64(0.440189077713297), 81: np.float64(0.34097142622121834), 91: np.float64(0.2647278476711539)}\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/1_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.eval()\n",
    "accuracy = test(model, test_dataloader, criterion)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"Saliency\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c22ed990-84f3-4d46-9575-76df04d97e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.55%\n",
      "The road score is:  {1: np.float64(0.9820257658315142), 11: np.float64(0.9274665111306769), 21: np.float64(0.8728924914795306), 31: np.float64(0.8038201046585481), 41: np.float64(0.7230428491972497), 51: np.float64(0.6394828207378183), 61: np.float64(0.5341273658014597), 71: np.float64(0.42564235011338775), 81: np.float64(0.33250923822599016), 91: np.float64(0.24350088953506518)}\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/2_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test(model, test_dataloader, criterion)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"Saliency\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b891c48-f2c1-489b-a15d-c10840417e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.63%\n",
      "The road score is:  {1: np.float64(0.9815523567285375), 11: np.float64(0.9356981658686606), 21: np.float64(0.8902092786831333), 31: np.float64(0.8415169642088424), 41: np.float64(0.7749564670168705), 51: np.float64(0.6978569255157439), 61: np.float64(0.60888594593442), 71: np.float64(0.5116983727967653), 81: np.float64(0.4039617397451793), 91: np.float64(0.3028486530926366)}\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/6_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test(model, test_dataloader, criterion)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"Saliency\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7679a22-4c8d-4907-bd5d-10d4c9723989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.43%\n",
      "The road score is:  {1: np.float64(0.9835630670525478), 11: np.float64(0.9342213103810895), 21: np.float64(0.8821054857545466), 31: np.float64(0.8255040473880926), 41: np.float64(0.7546564625140637), 51: np.float64(0.6619870188715149), 61: np.float64(0.5630498173438654), 71: np.float64(0.46577110960962625), 81: np.float64(0.3588333885456528), 91: np.float64(0.25681642034993374)}\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/12_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test(model, test_dataloader, criterion)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"Saliency\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "081e3664-e6e7-436f-aff5-c2f7aa424d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 82.70%\n",
      "The road score is:  {1: np.float64(0.9847261184524104), 11: np.float64(0.9267461343208111), 21: np.float64(0.8685318018749423), 31: np.float64(0.807615101667683), 41: np.float64(0.7248794158224089), 51: np.float64(0.6496159754595252), 61: np.float64(0.5688044644775477), 71: np.float64(0.475189554305934), 81: np.float64(0.3775650117210429), 91: np.float64(0.28272291065287086)}\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/18_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test(model, test_dataloader, criterion)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"Saliency\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71153524-114b-4145-96e9-faf0ff632e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.46%\n",
      "The road score is:  {1: np.float64(0.9860395021377726), 11: np.float64(0.9380568182055771), 21: np.float64(0.8859509869049661), 31: np.float64(0.8254678674765173), 41: np.float64(0.7475342621319393), 51: np.float64(0.651121103479216), 61: np.float64(0.5477292968705794), 71: np.float64(0.446885111033178), 81: np.float64(0.34203169419053664), 91: np.float64(0.24484063846912646)}\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/29_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "accuracy = test(model, test_dataloader, criterion)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"Saliency\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7648483-db6a-4abf-894e-4f85cd5d421e",
   "metadata": {},
   "source": [
    "## Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e470b08e-7dee-4a5c-aee7-4dbd292c2de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "274ae4ad-78f0-4e63-a585-ee071eea194a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 102.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(MODEL_PATH, map_location\u001b[38;5;241m=\u001b[39mdevice, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m result_normal \u001b[38;5;241m=\u001b[39m filter_and_compute_road(model, test_dataloader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntegratedGradients\u001b[39m\u001b[38;5;124m\"\u001b[39m, device, resnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe road score is: \u001b[39m\u001b[38;5;124m\"\u001b[39m, result_normal)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22452\\1191473941.py:31\u001b[0m, in \u001b[0;36mfilter_and_compute_road\u001b[1;34m(model, test_loader, method, device, resnet)\u001b[0m\n\u001b[0;32m     29\u001b[0m x_batch, y_batch \u001b[38;5;241m=\u001b[39m x_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# passing image through model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(x_batch)\n\u001b[0;32m     32\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# using only correct prediction\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GitHub\\NeuripsPruning\\code\\resnet_18.py:226\u001b[0m, in \u001b[0;36mResNet_features.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03mStandard forward pass through the network.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    225\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(inputs)\n\u001b[1;32m--> 226\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(inputs)\n\u001b[0;32m    227\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(inputs)\n\u001b[0;32m    228\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    202\u001b[0m     bn_training,\n\u001b[0;32m    203\u001b[0m     exponential_average_factor,\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    205\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2822\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2820\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m   2823\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2824\u001b[0m     weight,\n\u001b[0;32m   2825\u001b[0m     bias,\n\u001b[0;32m   2826\u001b[0m     running_mean,\n\u001b[0;32m   2827\u001b[0m     running_var,\n\u001b[0;32m   2828\u001b[0m     training,\n\u001b[0;32m   2829\u001b[0m     momentum,\n\u001b[0;32m   2830\u001b[0m     eps,\n\u001b[0;32m   2831\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled,\n\u001b[0;32m   2832\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 21.74 GiB is allocated by PyTorch, and 102.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/0_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "result_normal = filter_and_compute_road(model, test_dataloader, \"IntegratedGradients\", device, resnet = True)\n",
    "print(\"The road score is: \", result_normal)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb955b-65d6-42c3-8d25-aa3e6d0009fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/1_5_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "\n",
    "result_prune_5 = filter_and_compute_road(model, test_dataloader, \"IntegratedGradients\", device, resnet = True)\n",
    "print(\"The road score is: \", result_prune_5)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed0724-f712-41d1-badc-f0c124e00d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/1_10_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "\n",
    "result_prune_10 = filter_and_compute_road(model, test_dataloader, \"IntegratedGradients\", device, resnet = True)\n",
    "print(\"The road score is: \", result_prune_10)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402d023-4894-409f-ae08-2aa2e0cb4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/1_15_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "\n",
    "result_prune_15 = filter_and_compute_road(model, test_dataloader, \"IntegratedGradients\", device, resnet = True)\n",
    "print(\"The road score is: \", result_prune_15)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b701c1bb-d872-4f75-b94a-611d776ec02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/1_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "result_prune_20 = filter_and_compute_road(model, test_dataloader, \"IntegratedGradients\", device, resnet = True)\n",
    "print(\"The road score is: \", result_prune_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37deaea-0eac-48de-b304-0050173d6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/2_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "\n",
    "result_prune_30 = filter_and_compute_road(model, test_dataloader, \"IntegratedGradients\", device, resnet = True)\n",
    "print(\"The road score is: \", result_prune_30)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed0e8d5-5f06-4119-a9ab-b0aa300e9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/6_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "\n",
    "result_prune_40 = filter_and_compute_road(model, test_dataloader, \"IntegratedGradients\", device, resnet = True)\n",
    "print(\"The road score is: \", result_prune_40)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039eefc-3b0d-4ecf-aa39-893508b9b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/12_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "\n",
    "result_prune_50 = filter_and_compute_road(model, test_dataloader, \"IntegratedGradients\", device, resnet = True)\n",
    "print(\"The road score is: \", result_prune_50)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae663e-d581-4a43-a25a-358ac3a49c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/18_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "\n",
    "result_prune_60 = filter_and_compute_road(model, test_dataloader, \"IntegratedGradients\", device, resnet = True)\n",
    "print(\"The road score is: \", result_prune_60)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9b852-e7e5-4b79-9df0-55e191585680",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"saves/resnet/imagenette/29_model_lt.pth.tar\"\n",
    "model = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "model.to(device)\n",
    "\n",
    "result_prune_70 = filter_and_compute_road(model, test_dataloader, \"IntegratedGradients\", device, resnet = True)\n",
    "print(\"The road score is: \", result_prune_70)\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
