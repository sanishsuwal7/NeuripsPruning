{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4d93fb3-9f59-4cb5-b28e-4c70541e825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from ranger import Ranger\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torchmetrics import Accuracy\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torch.optim as optim\n",
    "from cleverhans.torch.attacks.projected_gradient_descent import (projected_gradient_descent)\n",
    "\n",
    "import quantus\n",
    "import captum\n",
    "from captum.attr import Saliency, IntegratedGradients, NoiseTunnel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5411ae1a-40b6-4acd-aa88-0b34cdeb5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "343e3a1b-ff0d-4cf8-a527-9898049992a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96559a2f-7593-42a5-a442-e37dbf157f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Style\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7592ddb0-dec1-4d6e-aa08-a3876b344ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2365d6a1-7d8e-49ef-9e7b-4890baeac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f46c8c15-3d03-4743-b01e-c187a67bdb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_path = '../datasets/imagenette2/train'\n",
    "val_path = '../datasets/imagenette2/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d343f9cc-2a33-49ff-bd6f-687aba9eddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(datasets.ImageFolder(train_path, transform = transforms.Compose([\n",
    "                                                                    transforms.RandomResizedCrop(224),\n",
    "                                                                    transforms.RandomHorizontalFlip(),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                                            ])), batch_size = batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(datasets.ImageFolder(val_path,\n",
    "                                                               transform=transforms.Compose([\n",
    "                                                                   transforms.ToTensor(),\n",
    "                                                                   transforms.Resize([224, 224]),\n",
    "                                                                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                                               ])),batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78be8633-7e41-43e7-af82-7ccf6fef1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('tench', 'springer', 'casette_player', 'chain_saw','church', 'French_horn', 'garbage_truck', 'gas_pump', 'golf_ball', 'parachute')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52c8ecd7-eae8-4584-9a17-5ba8144f4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd13e745-4cfd-4132-9343-0494ca84a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18_features(pretrained=False, filter='None', filter_layer=0).to(device)\n",
    "learning_rate = 1e-4\n",
    "start_iter = 0\n",
    "end_iter = 50\n",
    "print_freq = 1\n",
    "valid_freq = 1\n",
    "prune_type = 'lt'\n",
    "prune_percent = 10\n",
    "prune_iterations = 35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7257809-1787-4588-8008-f6c4c88ac982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet_features(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (global_pool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b02e0e46-84c5-4bcc-b65a-bb3b560a2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "checkdir(f\"{os.getcwd()}/saves/resnet/imagenette/\")\n",
    "torch.save(model, f\"{os.getcwd()}/saves/resnet/imagenette/initial_state_dict_lt.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b150d6e1-26e6-40f1-9e13-b6471283cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_mask(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65f97a4a-b344-42ef-aad6-fb8830db7e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()\n",
    "optimizer = Ranger(model.parameters(), weight_decay=1e-2, eps = 1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c535eb81-ccbb-4b34-9d86-71f23e55936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 7, 7])\n",
      "bn1.weight torch.Size([64])\n",
      "bn1.bias torch.Size([64])\n",
      "layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "layer2.0.bn1.weight torch.Size([128])\n",
      "layer2.0.bn1.bias torch.Size([128])\n",
      "layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([128])\n",
      "layer2.0.bn2.bias torch.Size([128])\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([128])\n",
      "layer2.1.bn1.bias torch.Size([128])\n",
      "layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([128])\n",
      "layer2.1.bn2.bias torch.Size([128])\n",
      "layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "layer3.0.bn1.weight torch.Size([256])\n",
      "layer3.0.bn1.bias torch.Size([256])\n",
      "layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([256])\n",
      "layer3.0.bn2.bias torch.Size([256])\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([256])\n",
      "layer3.1.bn1.bias torch.Size([256])\n",
      "layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([256])\n",
      "layer3.1.bn2.bias torch.Size([256])\n",
      "layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "layer4.0.bn1.weight torch.Size([512])\n",
      "layer4.0.bn1.bias torch.Size([512])\n",
      "layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([512])\n",
      "layer4.0.bn2.bias torch.Size([512])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn1.weight torch.Size([512])\n",
      "layer4.1.bn1.bias torch.Size([512])\n",
      "layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([512])\n",
      "layer4.1.bn2.bias torch.Size([512])\n",
      "fc.weight torch.Size([10, 512])\n",
      "fc.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "        print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "872e24ac-ae2e-4bd4-9a92-92b0dc3cd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning\n",
    "# NOTE First Pruning Iteration is of No Compression\n",
    "bestacc = 0.0\n",
    "best_accuracy = 0\n",
    "ITERATION = prune_iterations\n",
    "comp = np.zeros(ITERATION,float)\n",
    "bestacc = np.zeros(ITERATION,float)\n",
    "step = 0\n",
    "all_loss = np.zeros(end_iter,float)\n",
    "all_accuracy = np.zeros(end_iter,float)\n",
    "ITE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "618a24fd-115b-420c-88d4-2af28937d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [1:0/35]: ---\n",
      "conv1.weight         | nonzeros =    9408 /    9408 (100.00%) | total_pruned =       0 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   73728 /   73728 (100.00%) | total_pruned =       0 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    8192 /    8192 (100.00%) | total_pruned =       0 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  294912 /  294912 (100.00%) | total_pruned =       0 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   32768 /   32768 (100.00%) | total_pruned =       0 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros = 1179646 / 1179648 (100.00%) | total_pruned =       2 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =  131072 /  131072 (100.00%) | total_pruned =       0 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "fc.weight            | nonzeros =    5120 /    5120 (100.00%) | total_pruned =       0 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 11176840, pruned : 4802, total: 11181642, Compression rate :       1.00x  (  0.04% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49/50 Loss: 1.133616 Accuracy: 47.34% Best Accuracy: 62.19%: 100%|██████| 50/50 [1:07:42<00:00, 81.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "\n",
      "--- Pruning Level [1:1/35]: ---\n",
      "conv1.weight         | nonzeros =    8467 /    9408 ( 90.00%) | total_pruned =     941 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   66355 /   73728 ( 90.00%) | total_pruned =    7373 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    7372 /    8192 ( 89.99%) | total_pruned =     820 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  265420 /  294912 ( 90.00%) | total_pruned =   29492 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   29491 /   32768 ( 90.00%) | total_pruned =    3277 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros = 1061682 / 1179648 ( 90.00%) | total_pruned =  117966 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 2123367 / 2359296 ( 90.00%) | total_pruned =  235929 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =  117964 /  131072 ( 90.00%) | total_pruned =   13108 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "fc.weight            | nonzeros =    4608 /    5120 ( 90.00%) | total_pruned =     512 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 10059139, pruned : 1122503, total: 11181642, Compression rate :       1.11x  ( 10.04% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21/50 Loss: 1.444575 Accuracy: 53.20% Best Accuracy: 53.20%:  44%|███▌    | 22/50 [30:31<38:50, 83.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Frequency for Testing\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iter_ \u001b[38;5;241m%\u001b[39m valid_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 18\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m test(model, test_dataloader, criterion)\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m# Save Weights\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15172\\1220043021.py:11\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, test_loader, criterion)\u001b[0m\n\u001b[0;32m      9\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m---> 11\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# sum up batch loss\u001b[39;00m\n\u001b[0;32m     12\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# get the index of the max log-probability\u001b[39;00m\n\u001b[0;32m     13\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(target\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mview_as(pred))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ite in range(start_iter, ITERATION):\n",
    "    if not _ite == 0:\n",
    "        prune_by_percentile(prune_percent, resample=False, reinit=False)\n",
    "        original_initialization(mask, initial_state_dict)\n",
    "        optimizer = Ranger(model.parameters(), lr=learning_rate, weight_decay=1e-2, eps = 1e-06)\n",
    "        \n",
    "    print(f\"\\n--- Pruning Level [{ITE}:{_ite}/{ITERATION}]: ---\")\n",
    "\n",
    "    # Print the table of Nonzeros in each layer\n",
    "    comp1 = print_nonzeros(model)\n",
    "    comp[_ite] = comp1\n",
    "    pbar = tqdm(range(end_iter))\n",
    "\n",
    "    for iter_ in pbar:\n",
    "\n",
    "        # Frequency for Testing\n",
    "        if iter_ % valid_freq == 0:\n",
    "            accuracy = test(model, test_dataloader, criterion)\n",
    "\n",
    "            # Save Weights\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                checkdir(f\"{os.getcwd()}/saves/resnet/imagenette/\")\n",
    "                torch.save(model,f\"{os.getcwd()}/saves/resnet/imagenette/{_ite}_model_lt.pth.tar\")\n",
    "\n",
    "        # Training\n",
    "        loss = train(model, train_dataloader, optimizer, criterion)\n",
    "        all_loss[iter_] = loss\n",
    "        all_accuracy[iter_] = accuracy\n",
    "        \n",
    "        # Frequency for Printing Accuracy and Loss\n",
    "        if iter_ % print_freq == 0:\n",
    "            pbar.set_description(\n",
    "                f'Train Epoch: {iter_}/{end_iter} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')       \n",
    "\n",
    "    writer.add_scalar('Accuracy/test', best_accuracy, comp1)\n",
    "    bestacc[_ite]=best_accuracy\n",
    "\n",
    "    # Plotting Loss (Training), Accuracy (Testing), Iteration Curve\n",
    "    #NOTE Loss is computed for every iteration while Accuracy is computed only for every {args.valid_freq} iterations. Therefore Accuracy saved is constant during the uncomputed iterations.\n",
    "    #NOTE Normalized the accuracy to [0,100] for ease of plotting.\n",
    "    plt.plot(np.arange(1,(end_iter)+1), 100*(all_loss - np.min(all_loss))/np.ptp(all_loss).astype(float), c=\"blue\", label=\"Loss\") \n",
    "    plt.plot(np.arange(1,(end_iter)+1), all_accuracy, c=\"red\", label=\"Accuracy\") \n",
    "    plt.title(f\"Loss Vs Accuracy Vs Iterations (imagenette,resnet)\") \n",
    "    plt.xlabel(\"Iterations\") \n",
    "    plt.ylabel(\"Loss and Accuracy\") \n",
    "    plt.legend() \n",
    "    plt.grid(color=\"gray\") \n",
    "    checkdir(f\"{os.getcwd()}/plots/lt/resnet/imagenette/\")\n",
    "    plt.savefig(f\"{os.getcwd()}/plots/lt/resnet/imagenette/lt_LossVsAccuracy_{comp1}.png\", dpi=1200) \n",
    "    plt.close()\n",
    "\n",
    "    # Dump Plot values\n",
    "    checkdir(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/\")\n",
    "    all_loss.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_all_loss_{comp1}.dat\")\n",
    "    all_accuracy.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_all_accuracy_{comp1}.dat\")\n",
    "    \n",
    "    # Dumping mask\n",
    "    checkdir(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/\")\n",
    "    with open(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_mask_{comp1}.pkl\", 'wb') as fp:\n",
    "        pickle.dump(mask, fp)\n",
    "    \n",
    "    # Making variables into 0\n",
    "    best_accuracy = 0\n",
    "    all_loss = np.zeros(end_iter,float)\n",
    "    all_accuracy = np.zeros(end_iter,float)\n",
    "\n",
    "# Dumping Values for Plotting\n",
    "checkdir(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/\")\n",
    "comp.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_compression.dat\")\n",
    "bestacc.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_bestaccuracy.dat\")\n",
    "\n",
    "# Plotting\n",
    "a = np.arange(prune_iterations)\n",
    "plt.plot(a, bestacc, c=\"blue\", label=\"Winning tickets\") \n",
    "plt.title(f\"Test Accuracy vs Unpruned Weights Percentage (imagenette,resnet)\") \n",
    "plt.xlabel(\"Unpruned Weights Percentage\") \n",
    "plt.ylabel(\"test accuracy\") \n",
    "plt.xticks(a, comp, rotation =\"vertical\") \n",
    "plt.ylim(0,100)\n",
    "plt.legend() \n",
    "plt.grid(color=\"gray\") \n",
    "checkdir(f\"{os.getcwd()}/plots/lt/resnet/imagenette/\")\n",
    "plt.savefig(f\"{os.getcwd()}/plots/lt/resnet/imagenette/lt_AccuracyVsWeights.png\", dpi=1200) \n",
    "plt.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9ab49-8ac4-45ec-a195-e9f0a138b08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
