{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4d93fb3-9f59-4cb5-b28e-4c70541e825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from ranger import Ranger\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torchmetrics import Accuracy\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torch.optim as optim\n",
    "from cleverhans.torch.attacks.projected_gradient_descent import (projected_gradient_descent)\n",
    "\n",
    "import quantus\n",
    "import captum\n",
    "from captum.attr import Saliency, IntegratedGradients, NoiseTunnel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5411ae1a-40b6-4acd-aa88-0b34cdeb5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "343e3a1b-ff0d-4cf8-a527-9898049992a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96559a2f-7593-42a5-a442-e37dbf157f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Style\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7592ddb0-dec1-4d6e-aa08-a3876b344ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2365d6a1-7d8e-49ef-9e7b-4890baeac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f46c8c15-3d03-4743-b01e-c187a67bdb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_path = '../datasets/imagenette2/train'\n",
    "val_path = '../datasets/imagenette2/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d343f9cc-2a33-49ff-bd6f-687aba9eddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(datasets.ImageFolder(train_path, transform = transforms.Compose([\n",
    "                                                                    transforms.RandomResizedCrop(224),\n",
    "                                                                    transforms.RandomHorizontalFlip(),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                                            ])), batch_size = batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(datasets.ImageFolder(val_path,\n",
    "                                                               transform=transforms.Compose([\n",
    "                                                                   transforms.ToTensor(),\n",
    "                                                                   transforms.Resize([224, 224]),\n",
    "                                                                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                                               ])),batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78be8633-7e41-43e7-af82-7ccf6fef1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('tench', 'springer', 'casette_player', 'chain_saw','church', 'French_horn', 'garbage_truck', 'gas_pump', 'golf_ball', 'parachute')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52c8ecd7-eae8-4584-9a17-5ba8144f4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run resnet_18.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd13e745-4cfd-4132-9343-0494ca84a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet_18(pretrained=False, filter='None', filter_layer=0).to(device)\n",
    "learning_rate = 1e-3\n",
    "start_iter = 0\n",
    "end_iter = 100\n",
    "print_freq = 1\n",
    "valid_freq = 1\n",
    "prune_type = 'lt'\n",
    "prune_percent = 10\n",
    "prune_iterations = 35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e7257809-1787-4588-8008-f6c4c88ac982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b02e0e46-84c5-4bcc-b65a-bb3b560a2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "checkdir(f\"{os.getcwd()}/saves/resnet/imagenette/\")\n",
    "torch.save(model, f\"{os.getcwd()}/saves/resnet/imagenette/initial_state_dict_lt.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b150d6e1-26e6-40f1-9e13-b6471283cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_mask(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "65f97a4a-b344-42ef-aad6-fb8830db7e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()\n",
    "optimizer = Ranger(model.parameters(),lr = learning_rate, weight_decay=1e-4, eps = 1e-06)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=end_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c535eb81-ccbb-4b34-9d86-71f23e55936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 7, 7])\n",
      "bn1.weight torch.Size([64])\n",
      "bn1.bias torch.Size([64])\n",
      "layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "layer2.0.bn1.weight torch.Size([128])\n",
      "layer2.0.bn1.bias torch.Size([128])\n",
      "layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([128])\n",
      "layer2.0.bn2.bias torch.Size([128])\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([128])\n",
      "layer2.1.bn1.bias torch.Size([128])\n",
      "layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([128])\n",
      "layer2.1.bn2.bias torch.Size([128])\n",
      "layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "layer3.0.bn1.weight torch.Size([256])\n",
      "layer3.0.bn1.bias torch.Size([256])\n",
      "layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([256])\n",
      "layer3.0.bn2.bias torch.Size([256])\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([256])\n",
      "layer3.1.bn1.bias torch.Size([256])\n",
      "layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([256])\n",
      "layer3.1.bn2.bias torch.Size([256])\n",
      "layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "layer4.0.bn1.weight torch.Size([512])\n",
      "layer4.0.bn1.bias torch.Size([512])\n",
      "layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([512])\n",
      "layer4.0.bn2.bias torch.Size([512])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn1.weight torch.Size([512])\n",
      "layer4.1.bn1.bias torch.Size([512])\n",
      "layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([512])\n",
      "layer4.1.bn2.bias torch.Size([512])\n",
      "fc.weight torch.Size([10, 512])\n",
      "fc.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "        print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "872e24ac-ae2e-4bd4-9a92-92b0dc3cd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning\n",
    "# NOTE First Pruning Iteration is of No Compression\n",
    "bestacc = 0.0\n",
    "best_accuracy = 0\n",
    "ITERATION = prune_iterations\n",
    "comp = np.zeros(ITERATION,float)\n",
    "bestacc = np.zeros(ITERATION,float)\n",
    "step = 0\n",
    "all_loss = np.zeros(end_iter,float)\n",
    "all_accuracy = np.zeros(end_iter,float)\n",
    "ITE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "618a24fd-115b-420c-88d4-2af28937d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [1:0/35]: ---\n",
      "conv1.weight         | nonzeros =    9408 /    9408 (100.00%) | total_pruned =       0 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   73728 /   73728 (100.00%) | total_pruned =       0 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    8192 /    8192 (100.00%) | total_pruned =       0 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  294912 /  294912 (100.00%) | total_pruned =       0 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   32768 /   32768 (100.00%) | total_pruned =       0 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros = 1179648 / 1179648 (100.00%) | total_pruned =       0 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =  131072 /  131072 (100.00%) | total_pruned =       0 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "fc.weight            | nonzeros =    5120 /    5120 (100.00%) | total_pruned =       0 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 11176842, pruned : 4800, total: 11181642, Compression rate :       1.00x  (  0.04% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99/100 Loss: 0.124789 Accuracy: 87.39% Best Accuracy: 87.80%: 100%|███| 100/100 [1:53:43<00:00, 68.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "\n",
      "--- Pruning Level [1:1/35]: ---\n",
      "conv1.weight         | nonzeros =    8467 /    9408 ( 90.00%) | total_pruned =     941 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   66355 /   73728 ( 90.00%) | total_pruned =    7373 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    7372 /    8192 ( 89.99%) | total_pruned =     820 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  265420 /  294912 ( 90.00%) | total_pruned =   29492 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   29491 /   32768 ( 90.00%) | total_pruned =    3277 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros = 1061683 / 1179648 ( 90.00%) | total_pruned =  117965 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =  117964 /  131072 ( 90.00%) | total_pruned =   13108 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "fc.weight            | nonzeros =    4608 /    5120 ( 90.00%) | total_pruned =     512 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 10059139, pruned : 1122503, total: 11181642, Compression rate :       1.11x  ( 10.04% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 99/100 Loss: 0.209312 Accuracy: 88.05% Best Accuracy: 88.61%: 100%|███| 100/100 [1:55:05<00:00, 69.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "\n",
      "--- Pruning Level [1:2/35]: ---\n",
      "conv1.weight         | nonzeros =    7679 /    9408 ( 81.62%) | total_pruned =    1729 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "bn1.bias             | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   30587 /   36864 ( 82.97%) | total_pruned =    6277 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   30460 /   36864 ( 82.63%) | total_pruned =    6404 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   30423 /   36864 ( 82.53%) | total_pruned =    6441 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   30452 /   36864 ( 82.61%) | total_pruned =    6412 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   60498 /   73728 ( 82.06%) | total_pruned =   13230 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  120688 /  147456 ( 81.85%) | total_pruned =   26768 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    6908 /    8192 ( 84.33%) | total_pruned =    1284 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  120701 /  147456 ( 81.86%) | total_pruned =   26755 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  120922 /  147456 ( 82.01%) | total_pruned =   26534 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  240190 /  294912 ( 81.44%) | total_pruned =   54722 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  480918 /  589824 ( 81.54%) | total_pruned =  108906 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     229 /     256 ( 89.45%) | total_pruned =      27 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =     255 /     256 ( 99.61%) | total_pruned =       1 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   27302 /   32768 ( 83.32%) | total_pruned =    5466 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     229 /     256 ( 89.45%) | total_pruned =      27 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =     255 /     256 ( 99.61%) | total_pruned =       1 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  480348 /  589824 ( 81.44%) | total_pruned =  109476 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  481233 /  589824 ( 81.59%) | total_pruned =  108591 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     229 /     256 ( 89.45%) | total_pruned =      27 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =     255 /     256 ( 99.61%) | total_pruned =       1 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros =  958514 / 1179648 ( 81.25%) | total_pruned =  221134 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 1921696 / 2359296 ( 81.45%) | total_pruned =  437600 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     445 /     512 ( 86.91%) | total_pruned =      67 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =     495 /     512 ( 96.68%) | total_pruned =      17 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =  108017 /  131072 ( 82.41%) | total_pruned =   23055 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     445 /     512 ( 86.91%) | total_pruned =      67 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =     495 /     512 ( 96.68%) | total_pruned =      17 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 1917980 / 2359296 ( 81.29%) | total_pruned =  441316 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 1922502 / 2359296 ( 81.49%) | total_pruned =  436794 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     450 /     512 ( 87.89%) | total_pruned =      62 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =     500 /     512 ( 97.66%) | total_pruned =      12 | shape = (512,)\n",
      "fc.weight            | nonzeros =    4308 /    5120 ( 84.14%) | total_pruned =     812 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 9110971, pruned : 2070671, total: 11181642, Compression rate :       1.23x  ( 18.52% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 25/100 Loss: 0.448604 Accuracy: 84.99% Best Accuracy: 86.17%:  26%|█   | 26/100 [29:09<1:23:00, 67.30s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Frequency for Testing\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m iter_ \u001b[38;5;241m%\u001b[39m valid_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 18\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m test(model, test_dataloader, criterion)\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;66;03m# Save Weights\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16588\\1056110546.py:8\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(model, test_loader, criterion)\u001b[0m\n\u001b[0;32m      6\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m      9\u001b[0m         data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m         output \u001b[38;5;241m=\u001b[39m model(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresize(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:479\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[1;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:467\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# Define align_corners to avoid warnings\u001b[39;00m\n\u001b[0;32m    465\u001b[0m align_corners \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 467\u001b[0m img \u001b[38;5;241m=\u001b[39m interpolate(img, size\u001b[38;5;241m=\u001b[39msize, mode\u001b[38;5;241m=\u001b[39minterpolation, align_corners\u001b[38;5;241m=\u001b[39malign_corners, antialias\u001b[38;5;241m=\u001b[39mantialias)\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[0;32m    470\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:4678\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4676\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[1;32m-> 4678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_upsample_bilinear2d_aa(\n\u001b[0;32m   4679\u001b[0m         \u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors\n\u001b[0;32m   4680\u001b[0m     )\n\u001b[0;32m   4681\u001b[0m \u001b[38;5;66;03m# Two levels are necessary to prevent TorchScript from touching\u001b[39;00m\n\u001b[0;32m   4682\u001b[0m \u001b[38;5;66;03m# are_deterministic_algorithms_enabled.\u001b[39;00m\n\u001b[0;32m   4683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ite in range(start_iter, ITERATION):\n",
    "    if not _ite == 0:\n",
    "        prune_by_percentile(prune_percent, resample=False, reinit=False)\n",
    "        original_initialization(mask, initial_state_dict)\n",
    "        optimizer = Ranger(model.parameters(), lr=learning_rate, weight_decay=1e-4, eps = 1e-06)\n",
    "        \n",
    "    print(f\"\\n--- Pruning Level [{ITE}:{_ite}/{ITERATION}]: ---\")\n",
    "\n",
    "    # Print the table of Nonzeros in each layer\n",
    "    comp1 = print_nonzeros(model)\n",
    "    comp[_ite] = comp1\n",
    "    pbar = tqdm(range(end_iter))\n",
    "\n",
    "    for iter_ in pbar:\n",
    "\n",
    "        # Frequency for Testing\n",
    "        if iter_ % valid_freq == 0:\n",
    "            accuracy = test(model, test_dataloader, criterion)\n",
    "\n",
    "            # Save Weights\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                checkdir(f\"{os.getcwd()}/saves/resnet/imagenette/\")\n",
    "                torch.save(model,f\"{os.getcwd()}/saves/resnet/imagenette/{_ite}_model_lt.pth.tar\")\n",
    "\n",
    "        # Training\n",
    "        loss = train(model, train_dataloader, optimizer, criterion,scheduler)\n",
    "        all_loss[iter_] = loss\n",
    "        all_accuracy[iter_] = accuracy\n",
    "        \n",
    "        # Frequency for Printing Accuracy and Loss\n",
    "        if iter_ % print_freq == 0:\n",
    "            pbar.set_description(\n",
    "                f'Train Epoch: {iter_}/{end_iter} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')       \n",
    "\n",
    "    writer.add_scalar('Accuracy/test', best_accuracy, comp1)\n",
    "    bestacc[_ite]=best_accuracy\n",
    "\n",
    "    # Plotting Loss (Training), Accuracy (Testing), Iteration Curve\n",
    "    #NOTE Loss is computed for every iteration while Accuracy is computed only for every {args.valid_freq} iterations. Therefore Accuracy saved is constant during the uncomputed iterations.\n",
    "    #NOTE Normalized the accuracy to [0,100] for ease of plotting.\n",
    "    plt.plot(np.arange(1,(end_iter)+1), 100*(all_loss - np.min(all_loss))/np.ptp(all_loss).astype(float), c=\"blue\", label=\"Loss\") \n",
    "    plt.plot(np.arange(1,(end_iter)+1), all_accuracy, c=\"red\", label=\"Accuracy\") \n",
    "    plt.title(f\"Loss Vs Accuracy Vs Iterations (imagenette,resnet)\") \n",
    "    plt.xlabel(\"Iterations\") \n",
    "    plt.ylabel(\"Loss and Accuracy\") \n",
    "    plt.legend() \n",
    "    plt.grid(color=\"gray\") \n",
    "    checkdir(f\"{os.getcwd()}/plots/lt/resnet/imagenette/\")\n",
    "    plt.savefig(f\"{os.getcwd()}/plots/lt/resnet/imagenette/lt_LossVsAccuracy_{comp1}.png\", dpi=1200) \n",
    "    plt.close()\n",
    "\n",
    "    # Dump Plot values\n",
    "    checkdir(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/\")\n",
    "    all_loss.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_all_loss_{comp1}.dat\")\n",
    "    all_accuracy.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_all_accuracy_{comp1}.dat\")\n",
    "    \n",
    "    # Dumping mask\n",
    "    checkdir(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/\")\n",
    "    with open(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_mask_{comp1}.pkl\", 'wb') as fp:\n",
    "        pickle.dump(mask, fp)\n",
    "    \n",
    "    # Making variables into 0\n",
    "    best_accuracy = 0\n",
    "    all_loss = np.zeros(end_iter,float)\n",
    "    all_accuracy = np.zeros(end_iter,float)\n",
    "\n",
    "# Dumping Values for Plotting\n",
    "checkdir(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/\")\n",
    "comp.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_compression.dat\")\n",
    "bestacc.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_bestaccuracy.dat\")\n",
    "\n",
    "# Plotting\n",
    "a = np.arange(prune_iterations)\n",
    "plt.plot(a, bestacc, c=\"blue\", label=\"Winning tickets\") \n",
    "plt.title(f\"Test Accuracy vs Unpruned Weights Percentage (imagenette,resnet)\") \n",
    "plt.xlabel(\"Unpruned Weights Percentage\") \n",
    "plt.ylabel(\"test accuracy\") \n",
    "plt.xticks(a, comp, rotation =\"vertical\") \n",
    "plt.ylim(0,100)\n",
    "plt.legend() \n",
    "plt.grid(color=\"gray\") \n",
    "checkdir(f\"{os.getcwd()}/plots/lt/resnet/imagenette/\")\n",
    "plt.savefig(f\"{os.getcwd()}/plots/lt/resnet/imagenette/lt_AccuracyVsWeights.png\", dpi=1200) \n",
    "plt.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9ab49-8ac4-45ec-a195-e9f0a138b08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
