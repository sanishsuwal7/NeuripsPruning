{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d93fb3-9f59-4cb5-b28e-4c70541e825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from ranger import Ranger\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torchmetrics import Accuracy\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torch.optim as optim\n",
    "from cleverhans.torch.attacks.projected_gradient_descent import (projected_gradient_descent)\n",
    "\n",
    "import quantus\n",
    "import captum\n",
    "from captum.attr import Saliency, IntegratedGradients, NoiseTunnel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5411ae1a-40b6-4acd-aa88-0b34cdeb5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343e3a1b-ff0d-4cf8-a527-9898049992a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96559a2f-7593-42a5-a442-e37dbf157f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Style\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7592ddb0-dec1-4d6e-aa08-a3876b344ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2365d6a1-7d8e-49ef-9e7b-4890baeac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f46c8c15-3d03-4743-b01e-c187a67bdb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_path = '../datasets/imagenette2/train'\n",
    "val_path = '../datasets/imagenette2/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d343f9cc-2a33-49ff-bd6f-687aba9eddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(datasets.ImageFolder(train_path, transform = transforms.Compose([\n",
    "                                                                    transforms.RandomResizedCrop(224),\n",
    "                                                                    transforms.RandomHorizontalFlip(),\n",
    "                                                                    transforms.ToTensor(),\n",
    "                                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                                            ])), batch_size = batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(datasets.ImageFolder(val_path,\n",
    "                                                               transform=transforms.Compose([\n",
    "                                                                   transforms.ToTensor(),\n",
    "                                                                   transforms.Resize([224, 224]),\n",
    "                                                                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                                               ])),batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78be8633-7e41-43e7-af82-7ccf6fef1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('tench', 'springer', 'casette_player', 'chain_saw','church', 'French_horn', 'garbage_truck', 'gas_pump', 'golf_ball', 'parachute')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52c8ecd7-eae8-4584-9a17-5ba8144f4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd13e745-4cfd-4132-9343-0494ca84a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18_features(pretrained=False, filter='None', filter_layer=0).to(device)\n",
    "learning_rate = 1e-4\n",
    "start_iter = 0\n",
    "end_iter = 50\n",
    "print_freq = 1\n",
    "valid_freq = 1\n",
    "prune_type = 'lt'\n",
    "prune_percent = 10\n",
    "prune_iterations = 35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7257809-1787-4588-8008-f6c4c88ac982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet_features(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (global_pool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b02e0e46-84c5-4bcc-b65a-bb3b560a2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "checkdir(f\"{os.getcwd()}/saves/resnet/imagenette/\")\n",
    "torch.save(model, f\"{os.getcwd()}/saves/resnet/imagenette/initial_state_dict_lt.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b150d6e1-26e6-40f1-9e13-b6471283cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_mask(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65f97a4a-b344-42ef-aad6-fb8830db7e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "# criterion = nn.CrossEntropyLoss() \n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()\n",
    "optimizer = Ranger(model.parameters(), weight_decay=1e-2, eps = 1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c535eb81-ccbb-4b34-9d86-71f23e55936f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 7, 7])\n",
      "bn1.weight torch.Size([64])\n",
      "bn1.bias torch.Size([64])\n",
      "layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "layer2.0.bn1.weight torch.Size([128])\n",
      "layer2.0.bn1.bias torch.Size([128])\n",
      "layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([128])\n",
      "layer2.0.bn2.bias torch.Size([128])\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([128])\n",
      "layer2.1.bn1.bias torch.Size([128])\n",
      "layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([128])\n",
      "layer2.1.bn2.bias torch.Size([128])\n",
      "layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "layer3.0.bn1.weight torch.Size([256])\n",
      "layer3.0.bn1.bias torch.Size([256])\n",
      "layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([256])\n",
      "layer3.0.bn2.bias torch.Size([256])\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([256])\n",
      "layer3.1.bn1.bias torch.Size([256])\n",
      "layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([256])\n",
      "layer3.1.bn2.bias torch.Size([256])\n",
      "layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "layer4.0.bn1.weight torch.Size([512])\n",
      "layer4.0.bn1.bias torch.Size([512])\n",
      "layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([512])\n",
      "layer4.0.bn2.bias torch.Size([512])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn1.weight torch.Size([512])\n",
      "layer4.1.bn1.bias torch.Size([512])\n",
      "layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([512])\n",
      "layer4.1.bn2.bias torch.Size([512])\n",
      "fc.weight torch.Size([10, 512])\n",
      "fc.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "        print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "872e24ac-ae2e-4bd4-9a92-92b0dc3cd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning\n",
    "# NOTE First Pruning Iteration is of No Compression\n",
    "bestacc = 0.0\n",
    "best_accuracy = 0\n",
    "ITERATION = prune_iterations\n",
    "comp = np.zeros(ITERATION,float)\n",
    "bestacc = np.zeros(ITERATION,float)\n",
    "step = 0\n",
    "all_loss = np.zeros(end_iter,float)\n",
    "all_accuracy = np.zeros(end_iter,float)\n",
    "ITE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618a24fd-115b-420c-88d4-2af28937d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruning Level [1:0/35]: ---\n",
      "conv1.weight         | nonzeros =    9408 /    9408 (100.00%) | total_pruned =       0 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   36864 /   36864 (100.00%) | total_pruned =       0 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      64 /      64 (100.00%) | total_pruned =       0 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   73728 /   73728 (100.00%) | total_pruned =       0 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    8192 /    8192 (100.00%) | total_pruned =       0 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  147456 /  147456 (100.00%) | total_pruned =       0 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  294912 /  294912 (100.00%) | total_pruned =       0 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   32768 /   32768 (100.00%) | total_pruned =       0 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  589824 /  589824 (100.00%) | total_pruned =       0 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros = 1179648 / 1179648 (100.00%) | total_pruned =       0 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =  131072 /  131072 (100.00%) | total_pruned =       0 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 2359295 / 2359296 (100.00%) | total_pruned =       1 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 2359296 / 2359296 (100.00%) | total_pruned =       0 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     512 /     512 (100.00%) | total_pruned =       0 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "fc.weight            | nonzeros =    5120 /    5120 (100.00%) | total_pruned =       0 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 11176841, pruned : 4801, total: 11181642, Compression rate :       1.00x  (  0.04% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49/50 Loss: 1.108083 Accuracy: 65.27% Best Accuracy: 65.50%: 100%|██████| 50/50 [1:09:51<00:00, 83.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "\n",
      "--- Pruning Level [1:1/35]: ---\n",
      "conv1.weight         | nonzeros =    8467 /    9408 ( 90.00%) | total_pruned =     941 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "bn1.bias             | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   33177 /   36864 ( 90.00%) | total_pruned =    3687 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =       0 /      64 (  0.00%) | total_pruned =      64 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   66355 /   73728 ( 90.00%) | total_pruned =    7373 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    7372 /    8192 ( 89.99%) | total_pruned =     820 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  132710 /  147456 ( 90.00%) | total_pruned =   14746 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =       0 /     128 (  0.00%) | total_pruned =     128 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  265420 /  294912 ( 90.00%) | total_pruned =   29492 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   29491 /   32768 ( 90.00%) | total_pruned =    3277 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  530841 /  589824 ( 90.00%) | total_pruned =   58983 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =       0 /     256 (  0.00%) | total_pruned =     256 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros = 1061683 / 1179648 ( 90.00%) | total_pruned =  117965 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =  117964 /  131072 ( 90.00%) | total_pruned =   13108 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 2123365 / 2359296 ( 90.00%) | total_pruned =  235931 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 2123366 / 2359296 ( 90.00%) | total_pruned =  235930 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =       0 /     512 (  0.00%) | total_pruned =     512 | shape = (512,)\n",
      "fc.weight            | nonzeros =    4608 /    5120 ( 90.00%) | total_pruned =     512 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 10059138, pruned : 1122504, total: 11181642, Compression rate :       1.11x  ( 10.04% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49/50 Loss: 2.229920 Accuracy: 30.78% Best Accuracy: 33.66%: 100%|██████| 50/50 [1:08:29<00:00, 82.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "\n",
      "--- Pruning Level [1:2/35]: ---\n",
      "conv1.weight         | nonzeros =    7709 /    9408 ( 81.94%) | total_pruned =    1699 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "bn1.bias             | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   30319 /   36864 ( 82.25%) | total_pruned =    6545 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   30300 /   36864 ( 82.19%) | total_pruned =    6564 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =      63 /      64 ( 98.44%) | total_pruned =       1 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   30539 /   36864 ( 82.84%) | total_pruned =    6325 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   30332 /   36864 ( 82.28%) | total_pruned =    6532 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      51 /      64 ( 79.69%) | total_pruned =      13 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =      63 /      64 ( 98.44%) | total_pruned =       1 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   61175 /   73728 ( 82.97%) | total_pruned =   12553 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  122089 /  147456 ( 82.80%) | total_pruned =   25367 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =     122 /     128 ( 95.31%) | total_pruned =       6 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    6765 /    8192 ( 82.58%) | total_pruned =    1427 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =     122 /     128 ( 95.31%) | total_pruned =       6 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  121760 /  147456 ( 82.57%) | total_pruned =   25696 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  121478 /  147456 ( 82.38%) | total_pruned =   25978 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =     103 /     128 ( 80.47%) | total_pruned =      25 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  245265 /  294912 ( 83.17%) | total_pruned =   49647 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  485763 /  589824 ( 82.36%) | total_pruned =  104061 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =     238 /     256 ( 92.97%) | total_pruned =      18 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   27041 /   32768 ( 82.52%) | total_pruned =    5727 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =     238 /     256 ( 92.97%) | total_pruned =      18 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  483849 /  589824 ( 82.03%) | total_pruned =  105975 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  479772 /  589824 ( 81.34%) | total_pruned =  110052 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     207 /     256 ( 80.86%) | total_pruned =      49 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros =  964961 / 1179648 ( 81.80%) | total_pruned =  214687 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 1912987 / 2359296 ( 81.08%) | total_pruned =  446309 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =     504 /     512 ( 98.44%) | total_pruned =       8 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =  107916 /  131072 ( 82.33%) | total_pruned =   23156 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =     504 /     512 ( 98.44%) | total_pruned =       8 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 1912845 / 2359296 ( 81.08%) | total_pruned =  446451 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 1917376 / 2359296 ( 81.27%) | total_pruned =  441920 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     414 /     512 ( 80.86%) | total_pruned =      98 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =     510 /     512 ( 99.61%) | total_pruned =       2 | shape = (512,)\n",
      "fc.weight            | nonzeros =    4104 /    5120 ( 80.16%) | total_pruned =    1016 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 9112759, pruned : 2068883, total: 11181642, Compression rate :       1.23x  ( 18.50% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49/50 Loss: 1.972250 Accuracy: 41.43% Best Accuracy: 41.76%: 100%|██████| 50/50 [1:08:30<00:00, 82.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "\n",
      "--- Pruning Level [1:3/35]: ---\n",
      "conv1.weight         | nonzeros =    7094 /    9408 ( 75.40%) | total_pruned =    2314 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "bn1.bias             | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   27943 /   36864 ( 75.80%) | total_pruned =    8921 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   27906 /   36864 ( 75.70%) | total_pruned =    8958 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =      63 /      64 ( 98.44%) | total_pruned =       1 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   28462 /   36864 ( 77.21%) | total_pruned =    8402 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   28344 /   36864 ( 76.89%) | total_pruned =    8520 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      46 /      64 ( 71.88%) | total_pruned =      18 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =      63 /      64 ( 98.44%) | total_pruned =       1 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   57374 /   73728 ( 77.82%) | total_pruned =   16354 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  114410 /  147456 ( 77.59%) | total_pruned =   33046 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =     122 /     128 ( 95.31%) | total_pruned =       6 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    6342 /    8192 ( 77.42%) | total_pruned =    1850 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =     122 /     128 ( 95.31%) | total_pruned =       6 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  113612 /  147456 ( 77.05%) | total_pruned =   33844 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  113222 /  147456 ( 76.78%) | total_pruned =   34234 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =      92 /     128 ( 71.88%) | total_pruned =      36 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  230195 /  294912 ( 78.06%) | total_pruned =   64717 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  450118 /  589824 ( 76.31%) | total_pruned =  139706 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =     238 /     256 ( 92.97%) | total_pruned =      18 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   25006 /   32768 ( 76.31%) | total_pruned =    7762 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =     238 /     256 ( 92.97%) | total_pruned =      18 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  449022 /  589824 ( 76.13%) | total_pruned =  140802 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  438675 /  589824 ( 74.37%) | total_pruned =  151149 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     186 /     256 ( 72.66%) | total_pruned =      70 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros =  885458 / 1179648 ( 75.06%) | total_pruned =  294190 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 1738755 / 2359296 ( 73.70%) | total_pruned =  620541 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =     504 /     512 ( 98.44%) | total_pruned =       8 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =   98339 /  131072 ( 75.03%) | total_pruned =   32733 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =     504 /     512 ( 98.44%) | total_pruned =       8 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 1737723 / 2359296 ( 73.65%) | total_pruned =  621573 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 1729268 / 2359296 ( 73.30%) | total_pruned =  630028 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     372 /     512 ( 72.66%) | total_pruned =     140 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =     510 /     512 ( 99.61%) | total_pruned =       2 | shape = (512,)\n",
      "fc.weight            | nonzeros =    3799 /    5120 ( 74.20%) | total_pruned =    1321 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 8319086, pruned : 2862556, total: 11181642, Compression rate :       1.34x  ( 25.60% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49/50 Loss: 2.032919 Accuracy: 31.95% Best Accuracy: 31.95%: 100%|██████| 50/50 [1:08:30<00:00, 82.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "\n",
      "--- Pruning Level [1:4/35]: ---\n",
      "conv1.weight         | nonzeros =    6509 /    9408 ( 69.19%) | total_pruned =    2899 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "bn1.bias             | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   26183 /   36864 ( 71.03%) | total_pruned =   10681 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   26088 /   36864 ( 70.77%) | total_pruned =   10776 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =      63 /      64 ( 98.44%) | total_pruned =       1 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   26827 /   36864 ( 72.77%) | total_pruned =   10037 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   26529 /   36864 ( 71.96%) | total_pruned =   10335 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      41 /      64 ( 64.06%) | total_pruned =      23 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =      63 /      64 ( 98.44%) | total_pruned =       1 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   54091 /   73728 ( 73.37%) | total_pruned =   19637 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  108574 /  147456 ( 73.63%) | total_pruned =   38882 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =     122 /     128 ( 95.31%) | total_pruned =       6 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    5998 /    8192 ( 73.22%) | total_pruned =    2194 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =     122 /     128 ( 95.31%) | total_pruned =       6 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  106976 /  147456 ( 72.55%) | total_pruned =   40480 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  105998 /  147456 ( 71.88%) | total_pruned =   41458 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =      82 /     128 ( 64.06%) | total_pruned =      46 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  217184 /  294912 ( 73.64%) | total_pruned =   77728 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  421655 /  589824 ( 71.49%) | total_pruned =  168169 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =     238 /     256 ( 92.97%) | total_pruned =      18 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   23373 /   32768 ( 71.33%) | total_pruned =    9395 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =     238 /     256 ( 92.97%) | total_pruned =      18 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  421722 /  589824 ( 71.50%) | total_pruned =  168102 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  405153 /  589824 ( 68.69%) | total_pruned =  184671 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     167 /     256 ( 65.23%) | total_pruned =      89 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros =  821510 / 1179648 ( 69.64%) | total_pruned =  358138 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 1587148 / 2359296 ( 67.27%) | total_pruned =  772148 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =     504 /     512 ( 98.44%) | total_pruned =       8 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =   90569 /  131072 ( 69.10%) | total_pruned =   40503 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =     504 /     512 ( 98.44%) | total_pruned =       8 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 1593673 / 2359296 ( 67.55%) | total_pruned =  765623 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 1583736 / 2359296 ( 67.13%) | total_pruned =  775560 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     334 /     512 ( 65.23%) | total_pruned =     178 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =     510 /     512 ( 99.61%) | total_pruned =       2 | shape = (512,)\n",
      "fc.weight            | nonzeros =    3581 /    5120 ( 69.94%) | total_pruned =    1539 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 7670736, pruned : 3510906, total: 11181642, Compression rate :       1.46x  ( 31.40% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49/50 Loss: 1.799790 Accuracy: 45.63% Best Accuracy: 45.63%: 100%|██████| 50/50 [1:08:15<00:00, 81.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "\n",
      "--- Pruning Level [1:5/35]: ---\n",
      "conv1.weight         | nonzeros =    6031 /    9408 ( 64.11%) | total_pruned =    3377 | shape = (64, 3, 7, 7)\n",
      "bn1.weight           | nonzeros =      37 /      64 ( 57.81%) | total_pruned =      27 | shape = (64,)\n",
      "bn1.bias             | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.conv1.weight | nonzeros =   24981 /   36864 ( 67.77%) | total_pruned =   11883 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn1.weight  | nonzeros =      37 /      64 ( 57.81%) | total_pruned =      27 | shape = (64,)\n",
      "layer1.0.bn1.bias    | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.0.conv2.weight | nonzeros =   24825 /   36864 ( 67.34%) | total_pruned =   12039 | shape = (64, 64, 3, 3)\n",
      "layer1.0.bn2.weight  | nonzeros =      37 /      64 ( 57.81%) | total_pruned =      27 | shape = (64,)\n",
      "layer1.0.bn2.bias    | nonzeros =      63 /      64 ( 98.44%) | total_pruned =       1 | shape = (64,)\n",
      "layer1.1.conv1.weight | nonzeros =   25298 /   36864 ( 68.63%) | total_pruned =   11566 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn1.weight  | nonzeros =      37 /      64 ( 57.81%) | total_pruned =      27 | shape = (64,)\n",
      "layer1.1.bn1.bias    | nonzeros =      57 /      64 ( 89.06%) | total_pruned =       7 | shape = (64,)\n",
      "layer1.1.conv2.weight | nonzeros =   25189 /   36864 ( 68.33%) | total_pruned =   11675 | shape = (64, 64, 3, 3)\n",
      "layer1.1.bn2.weight  | nonzeros =      37 /      64 ( 57.81%) | total_pruned =      27 | shape = (64,)\n",
      "layer1.1.bn2.bias    | nonzeros =      63 /      64 ( 98.44%) | total_pruned =       1 | shape = (64,)\n",
      "layer2.0.conv1.weight | nonzeros =   51093 /   73728 ( 69.30%) | total_pruned =   22635 | shape = (128, 64, 3, 3)\n",
      "layer2.0.bn1.weight  | nonzeros =      73 /     128 ( 57.03%) | total_pruned =      55 | shape = (128,)\n",
      "layer2.0.bn1.bias    | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.0.conv2.weight | nonzeros =  102249 /  147456 ( 69.34%) | total_pruned =   45207 | shape = (128, 128, 3, 3)\n",
      "layer2.0.bn2.weight  | nonzeros =      73 /     128 ( 57.03%) | total_pruned =      55 | shape = (128,)\n",
      "layer2.0.bn2.bias    | nonzeros =     122 /     128 ( 95.31%) | total_pruned =       6 | shape = (128,)\n",
      "layer2.0.downsample.0.weight | nonzeros =    5761 /    8192 ( 70.32%) | total_pruned =    2431 | shape = (128, 64, 1, 1)\n",
      "layer2.0.downsample.1.weight | nonzeros =      73 /     128 ( 57.03%) | total_pruned =      55 | shape = (128,)\n",
      "layer2.0.downsample.1.bias | nonzeros =     122 /     128 ( 95.31%) | total_pruned =       6 | shape = (128,)\n",
      "layer2.1.conv1.weight | nonzeros =  101080 /  147456 ( 68.55%) | total_pruned =   46376 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn1.weight  | nonzeros =      73 /     128 ( 57.03%) | total_pruned =      55 | shape = (128,)\n",
      "layer2.1.bn1.bias    | nonzeros =     115 /     128 ( 89.84%) | total_pruned =      13 | shape = (128,)\n",
      "layer2.1.conv2.weight | nonzeros =  100288 /  147456 ( 68.01%) | total_pruned =   47168 | shape = (128, 128, 3, 3)\n",
      "layer2.1.bn2.weight  | nonzeros =      73 /     128 ( 57.03%) | total_pruned =      55 | shape = (128,)\n",
      "layer2.1.bn2.bias    | nonzeros =     128 /     128 (100.00%) | total_pruned =       0 | shape = (128,)\n",
      "layer3.0.conv1.weight | nonzeros =  204988 /  294912 ( 69.51%) | total_pruned =   89924 | shape = (256, 128, 3, 3)\n",
      "layer3.0.bn1.weight  | nonzeros =     150 /     256 ( 58.59%) | total_pruned =     106 | shape = (256,)\n",
      "layer3.0.bn1.bias    | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.0.conv2.weight | nonzeros =  397514 /  589824 ( 67.40%) | total_pruned =  192310 | shape = (256, 256, 3, 3)\n",
      "layer3.0.bn2.weight  | nonzeros =     150 /     256 ( 58.59%) | total_pruned =     106 | shape = (256,)\n",
      "layer3.0.bn2.bias    | nonzeros =     238 /     256 ( 92.97%) | total_pruned =      18 | shape = (256,)\n",
      "layer3.0.downsample.0.weight | nonzeros =   22210 /   32768 ( 67.78%) | total_pruned =   10558 | shape = (256, 128, 1, 1)\n",
      "layer3.0.downsample.1.weight | nonzeros =     150 /     256 ( 58.59%) | total_pruned =     106 | shape = (256,)\n",
      "layer3.0.downsample.1.bias | nonzeros =     238 /     256 ( 92.97%) | total_pruned =      18 | shape = (256,)\n",
      "layer3.1.conv1.weight | nonzeros =  395619 /  589824 ( 67.07%) | total_pruned =  194205 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn1.weight  | nonzeros =     150 /     256 ( 58.59%) | total_pruned =     106 | shape = (256,)\n",
      "layer3.1.bn1.bias    | nonzeros =     230 /     256 ( 89.84%) | total_pruned =      26 | shape = (256,)\n",
      "layer3.1.conv2.weight | nonzeros =  376954 /  589824 ( 63.91%) | total_pruned =  212870 | shape = (256, 256, 3, 3)\n",
      "layer3.1.bn2.weight  | nonzeros =     150 /     256 ( 58.59%) | total_pruned =     106 | shape = (256,)\n",
      "layer3.1.bn2.bias    | nonzeros =     256 /     256 (100.00%) | total_pruned =       0 | shape = (256,)\n",
      "layer4.0.conv1.weight | nonzeros =  769788 / 1179648 ( 65.26%) | total_pruned =  409860 | shape = (512, 256, 3, 3)\n",
      "layer4.0.bn1.weight  | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (512,)\n",
      "layer4.0.bn1.bias    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.0.conv2.weight | nonzeros = 1465301 / 2359296 ( 62.11%) | total_pruned =  893995 | shape = (512, 512, 3, 3)\n",
      "layer4.0.bn2.weight  | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (512,)\n",
      "layer4.0.bn2.bias    | nonzeros =     504 /     512 ( 98.44%) | total_pruned =       8 | shape = (512,)\n",
      "layer4.0.downsample.0.weight | nonzeros =   84685 /  131072 ( 64.61%) | total_pruned =   46387 | shape = (512, 256, 1, 1)\n",
      "layer4.0.downsample.1.weight | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (512,)\n",
      "layer4.0.downsample.1.bias | nonzeros =     504 /     512 ( 98.44%) | total_pruned =       8 | shape = (512,)\n",
      "layer4.1.conv1.weight | nonzeros = 1420577 / 2359296 ( 60.21%) | total_pruned =  938719 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn1.weight  | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (512,)\n",
      "layer4.1.bn1.bias    | nonzeros =     460 /     512 ( 89.84%) | total_pruned =      52 | shape = (512,)\n",
      "layer4.1.conv2.weight | nonzeros = 1438854 / 2359296 ( 60.99%) | total_pruned =  920442 | shape = (512, 512, 3, 3)\n",
      "layer4.1.bn2.weight  | nonzeros =     300 /     512 ( 58.59%) | total_pruned =     212 | shape = (512,)\n",
      "layer4.1.bn2.bias    | nonzeros =     510 /     512 ( 99.61%) | total_pruned =       2 | shape = (512,)\n",
      "fc.weight            | nonzeros =    3520 /    5120 ( 68.75%) | total_pruned =    1600 | shape = (10, 512)\n",
      "fc.bias              | nonzeros =      10 /      10 (100.00%) | total_pruned =       0 | shape = (10,)\n",
      "alive: 7054144, pruned : 4127498, total: 11181642, Compression rate :       1.59x  ( 36.91% pruned)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/50 Loss: 1.678967 Accuracy: 27.97% Best Accuracy: 27.97%:   4%|▎       | 2/50 [02:43<1:05:18, 81.63s/it]"
     ]
    }
   ],
   "source": [
    "for _ite in range(start_iter, ITERATION):\n",
    "    if not _ite == 0:\n",
    "        prune_by_percentile(prune_percent, resample=False, reinit=False)\n",
    "        original_initialization(mask, initial_state_dict)\n",
    "        optimizer = Ranger(model.parameters(), lr=learning_rate, weight_decay=1e-2, eps = 1e-06)\n",
    "        \n",
    "    print(f\"\\n--- Pruning Level [{ITE}:{_ite}/{ITERATION}]: ---\")\n",
    "\n",
    "    # Print the table of Nonzeros in each layer\n",
    "    comp1 = print_nonzeros(model)\n",
    "    comp[_ite] = comp1\n",
    "    pbar = tqdm(range(end_iter))\n",
    "\n",
    "    for iter_ in pbar:\n",
    "\n",
    "        # Frequency for Testing\n",
    "        if iter_ % valid_freq == 0:\n",
    "            accuracy = test(model, test_dataloader, criterion)\n",
    "\n",
    "            # Save Weights\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                checkdir(f\"{os.getcwd()}/saves/resnet/imagenette/\")\n",
    "                torch.save(model,f\"{os.getcwd()}/saves/resnet/imagenette/{_ite}_model_lt.pth.tar\")\n",
    "\n",
    "        # Training\n",
    "        loss = train(model, train_dataloader, optimizer, criterion)\n",
    "        all_loss[iter_] = loss\n",
    "        all_accuracy[iter_] = accuracy\n",
    "        \n",
    "        # Frequency for Printing Accuracy and Loss\n",
    "        if iter_ % print_freq == 0:\n",
    "            pbar.set_description(\n",
    "                f'Train Epoch: {iter_}/{end_iter} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')       \n",
    "\n",
    "    writer.add_scalar('Accuracy/test', best_accuracy, comp1)\n",
    "    bestacc[_ite]=best_accuracy\n",
    "\n",
    "    # Plotting Loss (Training), Accuracy (Testing), Iteration Curve\n",
    "    #NOTE Loss is computed for every iteration while Accuracy is computed only for every {args.valid_freq} iterations. Therefore Accuracy saved is constant during the uncomputed iterations.\n",
    "    #NOTE Normalized the accuracy to [0,100] for ease of plotting.\n",
    "    plt.plot(np.arange(1,(end_iter)+1), 100*(all_loss - np.min(all_loss))/np.ptp(all_loss).astype(float), c=\"blue\", label=\"Loss\") \n",
    "    plt.plot(np.arange(1,(end_iter)+1), all_accuracy, c=\"red\", label=\"Accuracy\") \n",
    "    plt.title(f\"Loss Vs Accuracy Vs Iterations (imagenette,resnet)\") \n",
    "    plt.xlabel(\"Iterations\") \n",
    "    plt.ylabel(\"Loss and Accuracy\") \n",
    "    plt.legend() \n",
    "    plt.grid(color=\"gray\") \n",
    "    checkdir(f\"{os.getcwd()}/plots/lt/resnet/imagenette/\")\n",
    "    plt.savefig(f\"{os.getcwd()}/plots/lt/resnet/imagenette/lt_LossVsAccuracy_{comp1}.png\", dpi=1200) \n",
    "    plt.close()\n",
    "\n",
    "    # Dump Plot values\n",
    "    checkdir(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/\")\n",
    "    all_loss.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_all_loss_{comp1}.dat\")\n",
    "    all_accuracy.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_all_accuracy_{comp1}.dat\")\n",
    "    \n",
    "    # Dumping mask\n",
    "    checkdir(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/\")\n",
    "    with open(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_mask_{comp1}.pkl\", 'wb') as fp:\n",
    "        pickle.dump(mask, fp)\n",
    "    \n",
    "    # Making variables into 0\n",
    "    best_accuracy = 0\n",
    "    all_loss = np.zeros(end_iter,float)\n",
    "    all_accuracy = np.zeros(end_iter,float)\n",
    "\n",
    "# Dumping Values for Plotting\n",
    "checkdir(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/\")\n",
    "comp.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_compression.dat\")\n",
    "bestacc.dump(f\"{os.getcwd()}/dumps/lt/resnet/imagenette/lt_bestaccuracy.dat\")\n",
    "\n",
    "# Plotting\n",
    "a = np.arange(prune_iterations)\n",
    "plt.plot(a, bestacc, c=\"blue\", label=\"Winning tickets\") \n",
    "plt.title(f\"Test Accuracy vs Unpruned Weights Percentage (imagenette,resnet)\") \n",
    "plt.xlabel(\"Unpruned Weights Percentage\") \n",
    "plt.ylabel(\"test accuracy\") \n",
    "plt.xticks(a, comp, rotation =\"vertical\") \n",
    "plt.ylim(0,100)\n",
    "plt.legend() \n",
    "plt.grid(color=\"gray\") \n",
    "checkdir(f\"{os.getcwd()}/plots/lt/resnet/imagenette/\")\n",
    "plt.savefig(f\"{os.getcwd()}/plots/lt/resnet/imagenette/lt_AccuracyVsWeights.png\", dpi=1200) \n",
    "plt.close()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9ab49-8ac4-45ec-a195-e9f0a138b08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
